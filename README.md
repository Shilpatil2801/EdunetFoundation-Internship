# ğŸ‡®ğŸ‡³ EdunetFoundation-Internship  
## Indian Sign Language Recognition System

**Real-time ISL Alphabet & Digit Recognition using MediaPipe + LSTM**

---

## ğŸ“Œ Project Overview

This project implements a **real-time Indian Sign Language (ISL) recognition system** capable of identifying **static alphabets (Aâ€“Z)** and **digits (0â€“9)** using:

- **MediaPipe Hand Landmarks** for feature extraction  
- **LSTM neural network** for temporal sequence learning  
- **Gradio** for interactive web-based real-time inference  
- **Hugging Face Spaces** for deployment  

The system works with **live webcam input** and provides **on-screen landmark visualization and prediction confidence**.

---

## ğŸ¯ Key Features

- ğŸ”´ Real-time webcam-based recognition  
- âœ‹ Hand landmark detection (21 points Ã— 3D)  
- ğŸ§  LSTM-based deep learning model  
- ğŸ“Š Confidence-based prediction filtering  
- ğŸ¥ Live landmark overlay  
- ğŸŒ Deployed on Hugging Face Spaces  
- ğŸ§ª Custom dataset (Aâ€“Z, 0â€“9)  

---

## ğŸ› ï¸ Tech Stack

| Category | Technology |
|--------|-----------|
| Language | Python |
| Hand Tracking | MediaPipe |
| Deep Learning | TensorFlow / Keras |
| Model | LSTM |
| UI | Gradio |
| Deployment | Hugging Face Spaces |
| Data Format | `.npy` landmark sequences |

---

## ğŸ“‚ Project Structure

```text
internshipproject/
â”‚
â”œâ”€â”€ app.py                  # Gradio application (real-time inference)
â”œâ”€â”€ camera.py               # MediaPipe hand landmark extraction
â”œâ”€â”€ inference_utils.py      # Model loading & prediction logic
â”œâ”€â”€ isl_model.h5            # Trained LSTM model (legacy)
â”œâ”€â”€ isl_model.keras         # Updated Keras model
â”œâ”€â”€ class_names.npy         # Label mapping (Aâ€“Z, 0â€“9)
â”œâ”€â”€ collectdata.py          # Dataset generation script
â”œâ”€â”€ isl.py                  # Training / experimentation script
â”œâ”€â”€ dataset/                # Custom landmark dataset
â”œâ”€â”€ requirements.txt        # Project dependencies
â”œâ”€â”€ README.md               # Project documentation
â””â”€â”€ .gitignore
```
## ğŸ§  Model Details

- **Input shape:** `(30, 126)`
  - 30 frames  
  - 21 landmarks Ã— (x, y, z) Ã— 2 hands  

- **Architecture:**
  - LSTM layers  
  - Dense output layer with Softmax  

- **Loss function:** Categorical Crossentropy  
- **Output:** Alphabet or digit label with confidence score  

---

## ğŸ“Š Dataset Description

- Custom-collected dataset using webcam  
- Each class folder contains `.npy` files  
- Each file represents a **sequence of hand landmarks**

### Labels include:
- **Digits:** `0â€“9`  
- **Alphabets:** `Aâ€“Z`  

---

## ğŸš€ Running Locally

### 1ï¸âƒ£ Create virtual environment
```bash
python -m venv isl_env
isl_env\Scripts\activate
### 2ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
### 3ï¸âƒ£ Run the application
```bash
python app.py

## ğŸŒ Deployment (Hugging Face Spaces)

- **Framework:** Gradio  
- **Runtime:** CPU  
- **Webcam access:** Enabled  
- **Public demo:** Accessible via browser  

---

## ğŸ“· Demo Capabilities

- Live webcam input  
- Real-time hand landmark visualization  
- Continuous prediction updates  
- Confidence thresholding to reduce false positives  

---

## âš ï¸ Known Limitations

- Static gestures only (no continuous word recognition yet)  
- Sensitive to lighting and camera angle  
- Single-hand dominant gestures work best  

---

## ğŸ”® Future Enhancements

- âœ… Dynamic gesture recognition (words/sentences)  
- âœ… Temporal smoothing for stable predictions  
- âœ… Multi-hand gesture support  
- âœ… Transformer-based sequence models  
- âœ… Mobile-friendly deployment  

---

## ğŸ‘©â€ğŸ’» Author

**Shilpa Patil**  
Artificial Intelligence & Data Science Student  
**Internship Project â€“ ISL Recognition**
